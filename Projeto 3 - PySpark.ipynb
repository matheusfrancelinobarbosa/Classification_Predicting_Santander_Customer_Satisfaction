{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b0fd9fe",
   "metadata": {},
   "source": [
    "# Predicting Santander Customer Satisfaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb9a19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Language Version\n",
    "from platform import python_version\n",
    "print('Python Language Version used in this Jupyter Notebook:', python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f52c5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import findspark and initialize\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4819ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import * \n",
    "from sklearn.metrics import confusion_matrix\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.classification import *\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.regression import *\n",
    "from pyspark.ml.evaluation import *\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder# Versions of packages used in this notebook jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5703db",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext watermark\n",
    "%watermark -a \"Matheus Francelino Barbosa\" --iversions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb453049",
   "metadata": {},
   "source": [
    "## Preparing the Spark Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e335289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark Context\n",
    "sc = SparkContext(appName = \"Mini-Projeto5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcac94f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d0a584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the session\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2151db9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a07b404",
   "metadata": {},
   "source": [
    "## Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a236cfe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the Dataset\n",
    "dados = spark.read.csv('dados/train.csv', inferSchema = True, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63be73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(dados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de217492",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bef434",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d66628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the data in the Pandas format\n",
    "dados.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9772178",
   "metadata": {},
   "outputs": [],
   "source": [
    "print((dados.count(), len(dados.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f635f0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema\n",
    "dados.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0663660",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove the column ID\n",
    "dados = dados.drop('ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc00f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema\n",
    "dados.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74402c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "colunas_dados = dados.columns[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8317fb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "colunas_test = ['var3','var15']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff15a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "colunas_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278a3710",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing to change the data type from Integer to Double\n",
    "for colunas in colunas_test:\n",
    "    if str(dados.schema[colunas].dataType) == 'IntegerType()':\n",
    "        dados_new = dados.withColumn(colunas,dados[colunas].cast(DoubleType()))\n",
    "        print((colunas,dados_new.schema[colunas].dataType))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b0de1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying the change from Integer to Double\n",
    "for colunas in colunas_dados:\n",
    "    \n",
    "    #if str(dados.schema[colunas].dataType) == 'DoubleType()':\n",
    "    dados = dados.withColumn(colunas,dados[colunas].cast(FloatType()))\n",
    "    #print((colunas,dados.schema[colunas].dataType))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f325857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema\n",
    "dados.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdd2074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the data in the Pandas format\n",
    "dados.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3aaf7a0",
   "metadata": {},
   "source": [
    "# Preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66a6ac5",
   "metadata": {},
   "source": [
    "### Checking for Null Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2235b39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the missing data (if any) and remove it (if any)\n",
    "dados_com_linhas_removidas = dados.na.drop()\n",
    "print('Number of rows before removing missing values:', dados.count())\n",
    "print('Number of rows after removing missing values:', dados_com_linhas_removidas.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39c913a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation function\n",
    "def func_modulo_prep_dados(df,\n",
    "                           variaveis_entrada,\n",
    "                           variavel_saida,\n",
    "                           tratar_outliers = True,\n",
    "                           padronizar_dados = True):\n",
    "\n",
    "    # Vamos gerar um novo dataframe, renomeando o argumento que representa a variável de saída.\n",
    "    novo_df = df.withColumnRenamed(variavel_saida, 'label')\n",
    "    \n",
    "    # Convertemos a variável alvo para o tipo numérico como float (encoding)\n",
    "    if str(novo_df.schema['label'].dataType) != 'IntegerType':\n",
    "        novo_df = novo_df.withColumn(\"label\", novo_df[\"label\"].cast(FloatType()))\n",
    "    \n",
    "    # Listas de controle para as variáveis\n",
    "    variaveis_numericas = []\n",
    "    variaveis_categoricas = []\n",
    "    \n",
    "    # Se tiver variáveis de entrada do tipo string, convertemos para o tipo numérico\n",
    "    for coluna in variaveis_entrada:\n",
    "        \n",
    "        # Verifica se a variável é do tipo string\n",
    "        if str(novo_df.schema[coluna].dataType) == 'StringType':\n",
    "            \n",
    "            # Definimos a variável com um sufixo\n",
    "            novo_nome_coluna = coluna + \"_num\"\n",
    "            \n",
    "            # Adicionamos à lista de variáveis categóricas\n",
    "            variaveis_categoricas.append(novo_nome_coluna)          \n",
    "            \n",
    "        else:\n",
    "            \n",
    "            # Se não for variável do tipo string, então é numérica e adicionamos na lista correspondente\n",
    "            variaveis_numericas.append(coluna)\n",
    "            \n",
    "            # Colocamos os dados no dataframe de variáveis indexadas\n",
    "            df_indexed = novo_df\n",
    "            \n",
    "    # Se o dataframe tiver dados do tipo string, aplicamos a indexação\n",
    "    # Verificamos se a lista de variáveis categóricas não está vazia\n",
    "    if len(variaveis_categoricas) != 0: \n",
    "        \n",
    "        # Loop pelas colunas\n",
    "        for coluna in novo_df:\n",
    "            \n",
    "            # Se a variável é do tipo string, criamos, treinamos e aplicamos o indexador\n",
    "            if str(novo_df.schema[coluna].dataType) == 'StringType':\n",
    "                \n",
    "                # Cria o indexador\n",
    "                indexer = StringIndexer(inputCol = coluna, outputCol = coluna + \"_num\") \n",
    "                \n",
    "                # Treina e aplica o indexador\n",
    "                df_indexed = indexer.fit(novo_df).transform(novo_df)\n",
    "    else:\n",
    "        \n",
    "        # Se não temos mais variáveis categóricas, então colocamos os dados no dataframe de variáveis indexadas\n",
    "        df_indexed = novo_df\n",
    "        \n",
    "    # Se for necessário tratar outliers, faremos isso agora\n",
    "    if tratar_outliers == True:\n",
    "        print(\"\\nAplicando o tratamento de outliers...\")\n",
    "        \n",
    "        # Dicionário\n",
    "        d = {}\n",
    "        \n",
    "        # Dicionário de quartis das variáveis do dataframe indexado (somente variáveis numéricas)\n",
    "        for col in variaveis_numericas: \n",
    "            d[col] = df_indexed.approxQuantile(col,[0.01, 0.99], 0.25) \n",
    "        \n",
    "        # Agora aplicamos transformação dependendo da distribuição de cada variável\n",
    "        for col in variaveis_numericas:\n",
    "            \n",
    "            # Extraímos a assimetria dos dados e usamos isso para tratar os outliers\n",
    "            skew = df_indexed.agg(skewness(df_indexed[col])).collect() \n",
    "            skew = skew[0][0]\n",
    "            \n",
    "            # Verificamos a assimetria e então aplicamos:\n",
    "            \n",
    "            # Transformação de log + 1 se a assimetria for positiva\n",
    "            if skew > 1:\n",
    "                indexed = df_indexed.withColumn(col, log(when(df[col] < d[col][0], d[col][0])\\\n",
    "                .when(df_indexed[col] > d[col][1], d[col][1])\\\n",
    "                .otherwise(df_indexed[col] ) + 1).alias(col))\n",
    "                print(\"\\nA variável \" + col + \" foi tratada para assimetria positiva (direita) com skew =\", skew)\n",
    "            \n",
    "            # Transformação exponencial se a assimetria for negativa\n",
    "            else:\n",
    "            #elif skew < -1:\n",
    "                indexed = df_indexed.withColumn(col, \\\n",
    "                exp(when(df[col] < d[col][0], d[col][0])\\\n",
    "                .when(df_indexed[col] > d[col][1], d[col][1])\\\n",
    "                .otherwise(df_indexed[col] )).alias(col))\n",
    "                print(\"\\nA variável \" + col + \" foi tratada para assimetria negativa (esquerda) com skew =\", skew)\n",
    "                \n",
    "            # Assimetria entre -1 e 1 não precisamos aplicar transformação aos dados\n",
    "\n",
    "    # Vetorização\n",
    "    \n",
    "    # Lista final de atributos\n",
    "    lista_atributos = variaveis_numericas + variaveis_categoricas\n",
    "    \n",
    "    # Cria o vetorizador para os atributos\n",
    "    vetorizador = VectorAssembler(inputCols = lista_atributos, outputCol = 'features')\n",
    "    \n",
    "    # Aplica o vetorizador ao conjunto de dados\n",
    "    dados_vetorizados = vetorizador.transform(df_indexed).select('features', 'label')\n",
    "    \n",
    "    # Se a flag padronizar_dados está como True, então padronizamos os dados colocando-os na mesma escala\n",
    "    if padronizar_dados == True:\n",
    "        print(\"\\nPadronizando o conjunto de dados para o intervalo de 0 a 1...\")\n",
    "        \n",
    "        # Cria o scaler\n",
    "        scaler = MinMaxScaler(inputCol = \"features\", outputCol = \"scaledFeatures\")\n",
    "\n",
    "        # Calcula o sumário de estatísticas e gera o padronizador\n",
    "        global scalerModel\n",
    "        scalerModel = scaler.fit(dados_vetorizados)\n",
    "\n",
    "        # Padroniza as variáveis para o intervalo [min, max]\n",
    "        dados_padronizados = scalerModel.transform(dados_vetorizados)\n",
    "        \n",
    "        # Gera os dados finais\n",
    "        dados_finais = dados_padronizados.select('label', 'scaledFeatures')\n",
    "        \n",
    "        # Renomeia as colunas (requerido pelo Spark)\n",
    "        dados_finais = dados_finais.withColumnRenamed('scaledFeatures', 'features')\n",
    "        \n",
    "        print(\"\\nProcesso Concluído!\")\n",
    "\n",
    "    # Se a flag está como False, então não padronizamos os dados\n",
    "    else:\n",
    "        print(\"\\nOs dados não serão padronizados pois a flag padronizar_dados está com o valor False.\")\n",
    "        dados_finais = dados_vetorizados\n",
    "    \n",
    "    return dados_finais\n",
    "\n",
    "# developed by Data Science Academy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d237cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of input variables (all but the last)\n",
    "variaveis_entrada = dados.columns[:-1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65c5793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target Variable\n",
    "variavel_saida = dados.columns[-1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416abccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function\n",
    "dados_finais = func_modulo_prep_dados(dados, variaveis_entrada, variavel_saida, False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e547b7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize\n",
    "dados_finais.show(10, truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39de5428",
   "metadata": {},
   "source": [
    "## Checking the Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113067bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the correlation\n",
    "coeficientes_corr = Correlation.corr(dados_finais, 'features', 'pearson').collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348aea4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the result into an array\n",
    "array_corr = coeficientes_corr.toArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8544885c",
   "metadata": {},
   "outputs": [],
   "source": [
    "array_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bc0b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the correlation between the attributes and the target variable\n",
    "for item in array_corr:\n",
    "    print(item[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038847fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(array_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8690a86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "array_test = array_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f71811",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(array_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083e3d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in array_test:\n",
    "    if 1 in array_test:\n",
    "        print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e30ccde",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1 in array_test: print('existe')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0747ac",
   "metadata": {},
   "source": [
    "## Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3be6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados_finais.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbca452a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados_finais.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f71e5ee",
   "metadata": {},
   "source": [
    "## Split into Training and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b566be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 70/30 ratio split\n",
    "dados_treino, dados_teste = dados_finais.randomSplit([0.7,0.3])\n",
    "print(\"Training Data Count:\" + str(dados_treino.count()))\n",
    "print(\"Test Data Count:\" + str(dados_teste.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b701f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados_treino.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18970262",
   "metadata": {},
   "source": [
    "## Balanced classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0035e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados_treino.filter(col(\"label\") == 1).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31dc94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados_treino.filter(col(\"label\") == 0).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57753b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_treino = dados_treino.toPandas()\n",
    "df_treino['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a225f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "minor_df = dados_treino.filter(col(\"label\") == 1)\n",
    "major_df = dados_treino.filter(col(\"label\") == 0)\n",
    "ratio = int(major_df.count()/minor_df.count())\n",
    "print(\"ratio: {}\".format(ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29e7f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = range(ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732e3f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "oversampled_df = minor_df.withColumn(\"dummy\", explode(array([lit(x) for x in a]))).drop('dummy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f819eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados_treino = major_df.unionAll(oversampled_df)\n",
    "dados_treino.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be08c36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados_treino.filter(col(\"label\") == 1).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c87cc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados_treino.filter(col(\"label\") == 0).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed32f0c0",
   "metadata": {},
   "source": [
    "# Applying Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f17b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the parameters\n",
    "lr = LogisticRegression(featuresCol = 'features', labelCol = 'label', maxIter=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6f6831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "modelo1 = lr.fit(dados_treino)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef821e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "predictions = modelo1.transform(dados_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4d5ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.select('label', 'features', 'rawPrediction', 'prediction', 'probability').toPandas().head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bfd3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = BinaryClassificationEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd19fa5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test Are Under ROC', evaluator.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381f1fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "accuracy = predictions.filter(predictions.label == predictions.prediction).count() / float(predictions.count())\n",
    "print(\"Accuracy : \",accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845546b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "beta = np.sort(modelo1.coefficients)\n",
    "plt.plot(beta)\n",
    "plt.ylabel('Beta Coefficients')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10ecb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingSummary = modelo1.summary\n",
    "roc = trainingSummary.roc.toPandas()\n",
    "plt.plot(roc['FPR'],roc['TPR'])\n",
    "plt.ylabel('False Positive Rate')\n",
    "plt.xlabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.show()\n",
    "print('Training set areaUnderROC: ' + str(trainingSummary.areaUnderROC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981f9214",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = trainingSummary.pr.toPandas()\n",
    "plt.plot(pr['recall'],pr['precision'])\n",
    "plt.ylabel('Precision')\n",
    "plt.xlabel('Recall')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c931d8",
   "metadata": {},
   "source": [
    "## Improving the previous model by setting the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6279348c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "paramGrid = (ParamGridBuilder()\n",
    "            .addGrid(lr.regParam, [0.01, 0.5, 2.0])\n",
    "            .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\n",
    "            .addGrid(lr.maxIter, [1,5,10])\n",
    "            .build())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320b6269",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CrossValidator(estimator=lr, \n",
    "                    estimatorParamMaps=paramGrid, \n",
    "                    evaluator=evaluator,\n",
    "                    numFolds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2232928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "cvModel = cv.fit(dados_treino)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9393c5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predictions\n",
    "predictions = cvModel.transform(dados_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52aa5213",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test Are Under ROC', evaluator.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eba726d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "accuracy = predictions.filter(predictions.label == predictions.prediction).count() / float(predictions.count())\n",
    "print(\"Accuracy : \",accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222208b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the confusion matrix\n",
    "y_pred=predictions.select(\"prediction\").collect()\n",
    "y_orig=predictions.select(\"label\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09405094",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_orig, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827f86f0",
   "metadata": {},
   "source": [
    "## Checking the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9623b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = cvModel.bestModel.coefficients\n",
    "weights = [(float(w),) for w in weights]\n",
    "weightsDF = spark.createDataFrame(weights, [\"Feature Weight\"])\n",
    "weightsDF.toPandas().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ace8f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = cvModel.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ea87d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best Param (regParam): ', best_model._java_obj.getRegParam())\n",
    "print('Best Param (MaxIter): ', best_model._java_obj.getMaxIter())\n",
    "print('Best Param (elasticNetParam): ', best_model._java_obj.getElasticNetParam())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15ef6b4",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d838100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the parameters\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6560865a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f4a24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "modelo2 = rf.fit(dados_treino)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa5bff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "predictionsRF = modelo2.transform(dados_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8730625d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test Are Under ROC', evaluator.evaluate(predictionsRF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9e3a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "accuracy = predictionsRF.filter(predictionsRF.label == predictionsRF.prediction).count() / float(predictionsRF.count())\n",
    "print(\"Accuracy : \",accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2a2c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the confusion matrix\n",
    "y_pred=predictionsRF.select(\"prediction\").collect()\n",
    "y_orig=predictionsRF.select(\"label\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254c2b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_orig, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a4406e",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9834c80f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set the parameters\n",
    "nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886e88e9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "modelo3 = nb.fit(dados_treino)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e98df8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "predictionsNB = modelo3.transform(dados_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ba7667",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionsNB.show(5, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bca0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test Are Under ROC', evaluator.evaluate(predictionsNB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33113363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "accuracy = predictionsNB.filter(predictionsNB.label == predictionsNB.prediction).count() / float(predictionsNB.count())\n",
    "print(\"Accuracy : \",accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477be690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the confusion matrix\n",
    "y_pred=predictionsNB.select(\"prediction\").collect()\n",
    "y_orig=predictionsNB.select(\"label\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d74d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_orig, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9537326",
   "metadata": {},
   "source": [
    "## GBT Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99f43c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the parameters\n",
    "gbt = GBTClassifier(maxIter=5, maxDepth=2, labelCol=\"label\", seed=42,\n",
    "    leafCol=\"leafId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece5be39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "modelo4 = gbt.fit(dados_treino)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323975ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "predictionsGBT = modelo4.transform(dados_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d853db",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionsGBT.show(3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e315c26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator=MulticlassClassificationEvaluator(metricName=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a318cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test Are Under ROC', evaluator.evaluate(predictionsGBT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616ac639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "acc = evaluator.evaluate(predictionsGBT)\n",
    "print(\"Prediction Accuracy: \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff1fc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the confusion matrix\n",
    "y_pred=predictionsGBT.select(\"prediction\").collect()\n",
    "y_orig=predictionsGBT.select(\"label\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17cbe9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_orig, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6661149",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6c5240",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
